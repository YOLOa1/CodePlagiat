# ============================================
# Dockerfile de Base - Cluster Spark/Hadoop
# ============================================
# Image optimisée contenant :
# - Java JDK 11
# - Apache Spark 3.3.0
# - Hadoop 3.3.1 (HDFS)
# - Python 3.9 avec dépendances
# - Outils pour analyse AST (tree-sitter, gcc, g++)
# ============================================

FROM debian:11-slim

# ============================================
# 1. INSTALLATION DES DÉPENDANCES SYSTÈME
# ============================================
# Installation des outils de base et compilateurs
RUN apt-get update && apt-get install -y --no-install-recommends \
    # Outils système essentiels
    curl \
    wget \
    vim \
    procps \
    net-tools \
    netcat \
    # Java Development Kit
    openjdk-11-jdk \
    # Compilateurs pour analyse de code (C++, Java)
    gcc \
    g++ \
    make \
    # Python et outils
    python3.9 \
    python3-pip \
    python3-dev \
    # Bibliothèques pour compilation native
    build-essential \
    libssl-dev \
    libffi-dev \
    # Nettoyage du cache pour réduire la taille de l'image
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# ============================================
# 2. CONFIGURATION DES VARIABLES D'ENVIRONNEMENT
# ============================================

# Java
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# Hadoop
ENV HADOOP_VERSION=3.3.1
ENV HADOOP_HOME=/opt/hadoop
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV HDFS_NAMENODE_USER=root
ENV HDFS_DATANODE_USER=root
ENV HDFS_SECONDARYNAMENODE_USER=root
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Spark
ENV SPARK_VERSION=3.3.0
ENV SPARK_HOME=/opt/spark
ENV SPARK_CONF_DIR=$SPARK_HOME/conf
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# Python
ENV PYTHONUNBUFFERED=1
ENV PYTHONIOENCODING=UTF-8

# ============================================
# 3. INSTALLATION DE HADOOP
# ============================================
RUN wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 --tries=5 \
    https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz \
    || wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 --tries=5 \
    https://dlcdn.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz \
    && tar -xzf hadoop-${HADOOP_VERSION}.tar.gz -C /opt/ \
    && mv /opt/hadoop-${HADOOP_VERSION} $HADOOP_HOME \
    && rm hadoop-${HADOOP_VERSION}.tar.gz

# Création des répertoires pour HDFS
RUN mkdir -p /hdfs/namenode \
    && mkdir -p /hdfs/datanode \
    && mkdir -p $HADOOP_HOME/logs

# ============================================
# 4. INSTALLATION D'APACHE SPARK
# ============================================
RUN wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 --tries=5 \
    https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz \
    || wget --retry-connrefused --waitretry=1 --read-timeout=20 --timeout=15 --tries=5 \
    https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz \
    && tar -xzf spark-${SPARK_VERSION}-bin-hadoop3.tgz -C /opt/ \
    && mv /opt/spark-${SPARK_VERSION}-bin-hadoop3 $SPARK_HOME \
    && rm spark-${SPARK_VERSION}-bin-hadoop3.tgz

# Création des répertoires de travail Spark
RUN mkdir -p $SPARK_HOME/logs \
    && mkdir -p /tmp/spark-events

# ============================================
# 5. INSTALLATION DES DÉPENDANCES PYTHON
# ============================================
# Mise à jour de pip et installation des outils essentiels
RUN pip3 install --no-cache-dir --upgrade pip setuptools wheel

# Installation des bibliothèques Python pour :
# - PySpark : Interface Python pour Spark
# - Tree-sitter : Parsing AST multi-langage
# - Analyse de code : AST, tokens
# - Manipulation de données : numpy, pandas
COPY docker/requirements.txt /tmp/requirements.txt
RUN pip3 install --no-cache-dir -r /tmp/requirements.txt

# ============================================
# 6. INSTALLATION DE TREE-SITTER ET GRAMMAIRES
# ============================================
# Tree-sitter pour parser Python, C++, Java
# Note: Les versions exactes peuvent varier selon la disponibilité
RUN pip3 install --no-cache-dir \
    tree-sitter || true

# ============================================
# 7. CONFIGURATION DES FICHIERS HADOOP/SPARK
# ============================================
# Les fichiers de config seront montés via volumes Docker
# Mais on crée des valeurs par défaut ici

# Configuration HDFS de base
RUN echo '<?xml version="1.0"?>\n\
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n\
<configuration>\n\
    <property>\n\
        <name>dfs.replication</name>\n\
        <value>1</value>\n\
    </property>\n\
    <property>\n\
        <name>dfs.namenode.name.dir</name>\n\
        <value>file:///hdfs/namenode</value>\n\
    </property>\n\
    <property>\n\
        <name>dfs.datanode.data.dir</name>\n\
        <value>file:///hdfs/datanode</value>\n\
    </property>\n\
    <property>\n\
        <name>dfs.permissions.enabled</name>\n\
        <value>false</value>\n\
    </property>\n\
</configuration>' > $HADOOP_CONF_DIR/hdfs-site.xml

# Configuration Core Hadoop
RUN echo '<?xml version="1.0"?>\n\
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>\n\
<configuration>\n\
    <property>\n\
        <name>fs.defaultFS</name>\n\
        <value>hdfs://spark-master:9000</value>\n\
    </property>\n\
    <property>\n\
        <name>hadoop.tmp.dir</name>\n\
        <value>/tmp/hadoop</value>\n\
    </property>\n\
</configuration>' > $HADOOP_CONF_DIR/core-site.xml

# Configuration Spark par défaut
RUN echo "spark.master=spark://spark-master:7077\n\
spark.eventLog.enabled=true\n\
spark.eventLog.dir=file:///tmp/spark-events\n\
spark.history.fs.logDirectory=file:///tmp/spark-events\n\
spark.executor.memory=1g\n\
spark.driver.memory=1g\n\
spark.sql.warehouse.dir=/tmp/spark-warehouse" > $SPARK_CONF_DIR/spark-defaults.conf

# ============================================
# 8. CRÉATION DE LA STRUCTURE DE RÉPERTOIRES
# ============================================
RUN mkdir -p /app/src/spark_jobs \
    && mkdir -p /app/src/utils \
    && mkdir -p /app/data/input \
    && mkdir -p /app/data/output \
    && mkdir -p /app/data/logs \
    && mkdir -p /app/configs

# ============================================
# 9. CONFIGURATION DU WORKDIR ET ENTRYPOINT
# ============================================
WORKDIR /app

# Copie du script d'entrée (sera créé séparément)
COPY docker/entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# ============================================
# 10. EXPOSITION DES PORTS
# ============================================
# Spark Master UI
EXPOSE 8080
# Spark Master (communication)
EXPOSE 7077
# Spark Worker UI
EXPOSE 8081
# HDFS NameNode UI
EXPOSE 9870
# HDFS NameNode (communication)
EXPOSE 9000
# HDFS DataNode
EXPOSE 9864

# ============================================
# 11. DÉFINITION DU POINT D'ENTRÉE
# ============================================
ENTRYPOINT ["/entrypoint.sh"]
