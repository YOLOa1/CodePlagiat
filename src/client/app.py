"""
============================================
Application Client Streamlit
============================================
Interface Web pour le syst√®me de d√©tection de plagiat.

Fonctionnalit√©s:
    - Upload de fichiers sources (Python, C++, Java)
    - Lancement de l'analyse distribu√©e
    - Visualisation des r√©sultats
    - T√©l√©chargement des rapports

Utilisation:
    streamlit run app.py --server.port=8501
============================================
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import os
import sys
from datetime import datetime
import time

# Ajouter le chemin src au PYTHONPATH
sys.path.append('/app/src')

from utils.hdfs_utils import HDFSClient
from utils.spark_utils import test_spark_connection


# ============================================
# CONFIGURATION DE LA PAGE
# ============================================
st.set_page_config(
    page_title="D√©tection de Plagiat - CodePlagiat",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================
# STYLE CSS PERSONNALIS√â
# ============================================
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.25rem;
        padding: 1rem;
        color: #155724;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeeba;
        border-radius: 0.25rem;
        padding: 1rem;
        color: #856404;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 0.25rem;
        padding: 1rem;
        color: #721c24;
    }
</style>
""", unsafe_allow_html=True)


# ============================================
# INITIALISATION DE LA SESSION
# ============================================
if 'hdfs_client' not in st.session_state:
    st.session_state.hdfs_client = HDFSClient("hdfs://spark-master:9000")

if 'uploaded_files' not in st.session_state:
    st.session_state.uploaded_files = []

if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None

if 'latest_job_type' not in st.session_state:
    st.session_state.latest_job_type = None

if 'latest_target_file' not in st.session_state:
    st.session_state.latest_target_file = None


# ============================================
# FONCTIONS UTILITAIRES
# ============================================

def check_system_health():
    """
    V√©rifie l'√©tat du syst√®me (Spark + HDFS).
    
    Returns:
        dict: √âtat des composants
    """
    health = {
        'spark': False,
        'hdfs': False
    }
    
    # Test Spark
    try:
        health['spark'] = test_spark_connection()
    except:
        health['spark'] = False
    
    # Test HDFS
    try:
        hdfs_client = st.session_state.hdfs_client
        health['hdfs'] = hdfs_client.exists('/')
    except:
        health['hdfs'] = False
    
    return health


def upload_to_hdfs(uploaded_file):
    """
    Upload un fichier vers HDFS.
    
    Args:
        uploaded_file: Fichier upload√© via Streamlit
        
    Returns:
        bool: True si succ√®s, False sinon
    """
    try:
        # Cr√©er un fichier temporaire
        temp_path = f"/tmp/{uploaded_file.name}"
        
        with open(temp_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # Upload vers HDFS
        hdfs_path = f"/app/input/{uploaded_file.name}"
        hdfs_client = st.session_state.hdfs_client
        
        success = hdfs_client.upload_file(temp_path, hdfs_path)
        
        # Nettoyer le fichier temporaire
        os.remove(temp_path)
        
        return success
    
    except Exception as e:
        st.error(f"Erreur lors de l'upload: {e}")
        return False


def submit_spark_job(target_file=None):
    """
    Soumet le job Spark pour analyse.
    
    Args:
        target_file: Nom du fichier cible pour comparaison cibl√©e (optionnel)
    
    Returns:
        bool: True si soumission r√©ussie, False sinon
    """
    try:
        import subprocess
        import os
        
        # Cr√©er l'archive des modules si elle n'existe pas
        zip_path = "/app/src/spark_jobs.zip"
        if not os.path.exists(zip_path):
            import shutil
            shutil.make_archive('/app/src/spark_jobs', 'zip', '/app/src', 'spark_jobs')
        
        # Timestamp pour un dossier unique
        timestamp = int(time.time())
        job_type = "target" if target_file else "all"
        output_path = f"hdfs://spark-master:9000/app/results/run_{job_type}_{timestamp}"
        
        # Commande spark-submit avec py-files
        command = [
            "/opt/spark/bin/spark-submit",
            "--master", "spark://spark-master:7077",
            "--deploy-mode", "client",
            "--driver-memory", "1g",
            "--executor-memory", "1g",
            "--executor-cores", "1",
            "--py-files", zip_path,
            "/app/src/spark_jobs/detect_plagiarism.py",
            "--input", "hdfs://spark-master:9000/app/input",
            "--output", output_path
        ]
        
        # Ajouter le param√®tre --target si sp√©cifi√©
        if target_file:
            command.extend(["--target", target_file])
        
        # Ex√©cuter et attendre la fin
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True
        )
        
        # Afficher la progression
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for line in process.stdout:
            # Afficher les lignes importantes
            if any(keyword in line for keyword in ['‚úÖ', 'üîç', 'üìä', 'üíæ', 'TERMIN√â', 'ERREUR']):
                status_text.text(line.strip())
            
            # Mettre √† jour la barre de progression bas√©e sur les √©tapes
            if 'Lecture des fichiers' in line:
                progress_bar.progress(20)
            elif 'Extraction des features' in line:
                progress_bar.progress(40)
            elif 'Comparaison par paires' in line:
                progress_bar.progress(60)
            elif 'Sauvegarde des r√©sultats' in line:
                progress_bar.progress(80)
            elif 'TERMIN√â AVEC SUCC√àS' in line:
                progress_bar.progress(100)
        
        process.wait()
        
        # Sauvegarder le chemin de sortie dans la session
        if process.returncode == 0:
            st.session_state.latest_output = output_path
            st.session_state.latest_job_type = "target" if target_file else "all"
            st.session_state.latest_target_file = target_file
            return True
        else:
            st.error(f"Le job a √©chou√© avec le code: {process.returncode}")
            return False
    
    except Exception as e:
        st.error(f"Erreur lors de la soumission: {e}")
        import traceback
        st.error(traceback.format_exc())
        return False


def get_hdfs_files():
    """
    R√©cup√®re la liste des fichiers disponibles dans HDFS /app/input.
    
    Returns:
        list: Liste des noms de fichiers
    """
    try:
        import subprocess
        
        result = subprocess.run(
            ['hdfs', 'dfs', '-ls', '/app/input'],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            return []
        
        # Extraire les noms de fichiers
        files = []
        for line in result.stdout.strip().split('\n'):
            if '/app/input/' in line and not line.strip().startswith('Found'):
                parts = line.split()
                if len(parts) > 0:
                    filepath = parts[-1]
                    filename = filepath.split('/')[-1]
                    if filename and filename.strip() and not filename.startswith('.'):
                        files.append(filename.strip())
        
        return sorted(files)
    
    except Exception as e:
        st.error(f"Erreur lors de la r√©cup√©ration des fichiers HDFS: {e}")
        return []


def count_hdfs_files():
    """
    Compte le nombre de fichiers dans HDFS /app/input.
    
    Returns:
        int: Nombre de fichiers
    """
    return len(get_hdfs_files())


def delete_latest_results():
    """
    Supprime les derniers r√©sultats d'analyse de HDFS.
    
    Returns:
        bool: True si succ√®s, False sinon
    """
    try:
        import subprocess
        
        # Lister les dossiers dans /app/results
        result = subprocess.run(
            ['hdfs', 'dfs', '-ls', '/app/results'],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0 or not result.stdout.strip():
            return False
        
        # Extraire le dossier le plus r√©cent (run_*)
        lines = result.stdout.strip().split('\n')
        run_dirs = []
        for line in lines:
            if 'run_' in line:
                parts = line.split()
                if len(parts) > 0:
                    run_dirs.append(parts[-1])
        
        if not run_dirs:
            return False
        
        # Prendre le plus r√©cent
        latest_dir = sorted(run_dirs)[-1]
        
        # Supprimer le dossier
        result = subprocess.run(
            ['hdfs', 'dfs', '-rm', '-r', latest_dir],
            capture_output=True,
            text=True
        )
        
        return result.returncode == 0
    
    except Exception as e:
        st.error(f"Erreur lors de la suppression: {e}")
        return False


def load_results():
    """
    Charge les r√©sultats depuis HDFS.
    
    Returns:
        pd.DataFrame: DataFrame des r√©sultats, None si erreur
    """
    try:
        import subprocess
        import io
        
        # Utiliser le dernier output si disponible, sinon chercher le plus r√©cent
        if hasattr(st.session_state, 'latest_output') and st.session_state.latest_output:
            result_path = st.session_state.latest_output.replace('hdfs://spark-master:9000', '')
        else:
            # Lister les dossiers dans /app/results
            result = subprocess.run(
                ['hdfs', 'dfs', '-ls', '/app/results'],
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0 or not result.stdout.strip():
                st.warning("Aucun dossier de r√©sultats trouv√© dans HDFS")
                return None
            
            # Extraire le dossier le plus r√©cent (run_*)
            lines = result.stdout.strip().split('\n')
            run_dirs = [line.split()[-1] for line in lines if 'run_' in line]
            
            if not run_dirs:
                st.warning("Aucun dossier run_* trouv√©")
                return None
            
            result_path = sorted(run_dirs)[-1]
        
        st.info(f"üìÇ Chargement depuis: {result_path}")
        
        # Lire directement le contenu CSV avec hdfs dfs -cat
        # Utiliser un pattern pour capturer tous les fichiers part-*.csv
        csv_pattern = f"{result_path}/part-*.csv"
        
        result = subprocess.run(
            ['hdfs', 'dfs', '-cat', csv_pattern],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            st.error(f"Erreur lors de la lecture du CSV: {result.stderr}")
            return None
        
        # V√©rifier si on a du contenu
        csv_content = result.stdout.strip()
        if not csv_content or csv_content == "file1,file2,similarity_score,detection_time":
            st.warning("Le fichier de r√©sultats est vide (aucune similarit√© d√©tect√©e)")
            return None
        
        # Charger le CSV depuis le contenu texte
        df = pd.read_csv(io.StringIO(csv_content))
        
        # V√©rifier si le dataframe est vide
        if df.empty:
            st.warning("Aucune similarit√© d√©tect√©e dans les r√©sultats")
            return None
        
        # Nettoyer les noms de fichiers pour affichage (garder juste le nom)
        if 'file1' in df.columns:
            df['file1'] = df['file1'].apply(lambda x: x.split('/')[-1] if isinstance(x, str) and '/' in x else x)
        if 'file2' in df.columns:
            df['file2'] = df['file2'].apply(lambda x: x.split('/')[-1] if isinstance(x, str) and '/' in x else x)
        
        return df
    
    except Exception as e:
        st.error(f"Erreur lors du chargement des r√©sultats: {e}")
        import traceback
        st.error(traceback.format_exc())
        return None


# ============================================
# INTERFACE PRINCIPALE
# ============================================

def main():
    """
    Interface principale de l'application.
    """
    
    # ========================================
    # HEADER
    # ========================================
    st.markdown('<div class="main-header">üîç D√©tection de Plagiat Distribu√©</div>', 
                unsafe_allow_html=True)
    st.markdown('<div class="sub-header">Analyse de code source avec Apache Spark</div>', 
                unsafe_allow_html=True)
    
    # ========================================
    # SIDEBAR - √âTAT DU SYST√àME
    # ========================================
    with st.sidebar:
        st.header("‚öôÔ∏è √âtat du Syst√®me")
        
        if st.button("üîÑ V√©rifier la sant√© du syst√®me"):
            with st.spinner("V√©rification en cours..."):
                health = check_system_health()
            
            if health['spark']:
                st.success("‚úÖ Spark Master: Op√©rationnel")
            else:
                st.error("‚ùå Spark Master: Indisponible")
            
            if health['hdfs']:
                st.success("‚úÖ HDFS: Op√©rationnel")
            else:
                st.error("‚ùå HDFS: Indisponible")
        
        st.markdown("---")
        
        st.header("üìä Statistiques")
        hdfs_file_count = count_hdfs_files()
        st.metric("Fichiers dans HDFS", hdfs_file_count)
        
        if st.button("üîÑ Actualiser"):
            st.rerun()
        
        st.markdown("---")
        
        st.header("‚ÑπÔ∏è √Ä propos")
        st.info("""
        **CodePlagiat v1.0**
        
        Syst√®me de d√©tection de plagiat distribu√© utilisant:
        - Apache Spark 3.3.0
        - Hadoop HDFS 3.3.1
        - Algorithme Winnowing
        - Similarit√© Jaccard
        """)
    
    # ========================================
    # ONGLETS PRINCIPAUX
    # ========================================
    tab1, tab2, tab3 = st.tabs(["üì§ Upload & Analyse", "üìä R√©sultats", "üìñ Documentation"])
    
    # ========================================
    # ONGLET 1: UPLOAD & ANALYSE
    # ========================================
    with tab1:
        st.header("üì§ Upload de fichiers sources")
        
        st.info("""
        **Instructions:**
        1. Uploadez vos fichiers sources (Python, C++, Java)
        2. V√©rifiez la liste des fichiers
        3. Lancez l'analyse distribu√©e
        """)
        
        # Upload de fichiers
        uploaded_files = st.file_uploader(
            "S√©lectionnez les fichiers √† analyser",
            type=['py', 'cpp', 'c', 'h', 'java'],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            st.success(f"‚úÖ {len(uploaded_files)} fichier(s) s√©lectionn√©(s)")
            
            # Afficher la liste
            with st.expander("üìã Voir la liste des fichiers"):
                for f in uploaded_files:
                    st.write(f"- {f.name} ({f.size} octets)")
            
            # Bouton d'upload vers HDFS
            if st.button("‚¨ÜÔ∏è Upload vers HDFS", type="primary"):
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                for i, file in enumerate(uploaded_files):
                    status_text.text(f"Upload de {file.name}...")
                    success = upload_to_hdfs(file)
                    
                    if success:
                        st.session_state.uploaded_files.append(file.name)
                    
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                status_text.text("‚úÖ Upload termin√©!")
                st.success("Tous les fichiers ont √©t√© upload√©s vers HDFS")
        
        st.markdown("---")
        
        # Lancement de l'analyse
        st.header("üöÄ Lancement de l'analyse")
        
        st.info("""
        **Analyse compl√®te**: Compare tous les fichiers entre eux pour d√©tecter les similarit√©s.
        """)
        
        # V√©rifier les fichiers dans HDFS
        hdfs_files = get_hdfs_files()
        hdfs_file_count = len(hdfs_files)
        
        # V√©rification avant lancement
        if hdfs_file_count < 2:
            st.warning(f"‚ö†Ô∏è Veuillez uploader au moins 2 fichiers pour d√©marrer l'analyse (actuellement: {hdfs_file_count} fichier(s) dans HDFS)")
            
            if hdfs_file_count > 0:
                with st.expander("üìã Fichiers disponibles dans HDFS"):
                    for f in hdfs_files:
                        st.write(f"- {f}")
        else:
            st.success(f"üìÅ {hdfs_file_count} fichiers pr√™ts pour l'analyse dans HDFS")
            
            with st.expander("üìã Voir la liste des fichiers"):
                for f in hdfs_files:
                    st.write(f"- {f}")
            
            if st.button("üîç Analyser tous les fichiers", type="primary"):
                with st.spinner("üîÑ Soumission du job Spark en cours..."):
                    success = submit_spark_job()
                
                if success:
                    st.success("‚úÖ Job Spark soumis avec succ√®s!")
                    st.info("""
                    L'analyse compl√®te est en cours d'ex√©cution sur le cluster Spark.
                    Consultez l'onglet **R√©sultats** dans quelques instants.
                    """)
                    
                    # Lien vers Spark UI
                    st.markdown("[üåê Voir le statut sur Spark UI](http://localhost:8080)")
                else:
                    st.error("‚ùå Erreur lors de la soumission du job")
    
    # ========================================
    # ONGLET 2: R√âSULTATS
    # ========================================
    with tab2:
        st.header("üìä R√©sultats de l'analyse")
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            if st.button("üîÑ Charger les r√©sultats", use_container_width=True):
                with st.spinner("Chargement des r√©sultats depuis HDFS..."):
                    df = load_results()
                
                if df is not None:
                    st.session_state.analysis_results = df
                    st.success("‚úÖ R√©sultats charg√©s!")
                else:
                    st.warning("‚ö†Ô∏è Aucun r√©sultat disponible. Lancez d'abord une analyse.")
        
        with col2:
            if st.button("üóëÔ∏è Supprimer", use_container_width=True, help="Supprimer les derniers r√©sultats"):
                with st.spinner("Suppression en cours..."):
                    success = delete_latest_results()
                
                if success:
                    st.success("‚úÖ R√©sultats supprim√©s!")
                    st.session_state.analysis_results = None
                    st.rerun()
                else:
                    st.error("‚ùå Aucun r√©sultat √† supprimer ou erreur")
        
        st.markdown("---")
        
        # Affichage des r√©sultats
        if st.session_state.analysis_results is not None:
            df = st.session_state.analysis_results
            
            # Section de filtrage par fichier en haut
            st.subheader("üîç Filtrer les r√©sultats par fichier")
            
            # R√©cup√©rer la liste des fichiers uniques
            all_files = sorted(set(df['file1'].tolist() + df['file2'].tolist()))
            
            filter_options = ["Tous les r√©sultats"] + all_files
            
            selected_filter = st.selectbox(
                "Afficher les comparaisons pour:",
                options=filter_options,
                help="S√©lectionnez un fichier pour voir uniquement ses comparaisons",
                key="file_filter_selector"
            )
            
            # Filtrer le dataframe si un fichier est s√©lectionn√©
            if selected_filter != "Tous les r√©sultats":
                filtered_df = df[
                    (df['file1'] == selected_filter) | (df['file2'] == selected_filter)
                ].copy()
                
                # Cr√©er une colonne "Autre fichier" pour simplifier l'affichage
                filtered_df['other_file'] = filtered_df.apply(
                    lambda row: row['file2'] if row['file1'] == selected_filter else row['file1'],
                    axis=1
                )
                
                # Trier par score d√©croissant
                filtered_df = filtered_df.sort_values('similarity_score', ascending=False)
                
                st.info(f"üìÑ Affichage des comparaisons pour: **{selected_filter}**")
            else:
                filtered_df = df
                st.info("üìä Affichage de toutes les comparaisons")
            
            st.markdown("---")
            
            # Statistiques globales
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Comparaisons affich√©es", len(filtered_df))
            
            with col2:
                plagiarism_count = len(filtered_df[filtered_df['similarity_score'] >= 0.7])
                st.metric("Plagiats d√©tect√©s (>70%)", plagiarism_count)
            
            with col3:
                avg_similarity = filtered_df['similarity_score'].mean()
                st.metric("Similarit√© moyenne", f"{avg_similarity:.2%}")
            
            # Graphique de distribution
            st.subheader("üìà Distribution des similarit√©s")
            
            fig = px.histogram(
                filtered_df,
                x='similarity_score',
                nbins=20,
                title="Distribution des scores de similarit√©",
                labels={'similarity_score': 'Score de similarit√©', 'count': 'Nombre de paires'}
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Top 10 des similarit√©s
            st.subheader("üèÜ Top 10 des similarit√©s")
            
            top_10 = filtered_df.nlargest(10, 'similarity_score')
            
            # Ajouter un indicateur visuel pour les plagiats
            def highlight_plagiarism(row):
                if row['similarity_score'] >= 0.95:
                    return ['background-color: #ff6b6b'] * len(row)
                elif row['similarity_score'] >= 0.80:
                    return ['background-color: #ffa07a'] * len(row)
                elif row['similarity_score'] >= 0.70:
                    return ['background-color: #fff4a3'] * len(row)
                else:
                    return [''] * len(row)
            
            # Pr√©parer le dataframe pour le top 10
            if selected_filter != "Tous les r√©sultats":
                # Si filtr√©, afficher avec la colonne other_file
                top_10_display = top_10[['other_file', 'similarity_score']].copy()
                top_10_display.columns = ['Fichier compar√©', 'Score de similarit√©']
                top_10_display['Score de similarit√©'] = top_10_display['Score de similarit√©'].apply(lambda x: f"{x:.2%}")
            else:
                # Sinon afficher file1 et file2
                top_10_display = top_10[['file1', 'file2', 'similarity_score']].copy()
                top_10_display['similarity_score'] = top_10_display['similarity_score'].apply(lambda x: f"{x:.2%}")
            
            st.dataframe(top_10_display, use_container_width=True, hide_index=True)
            
            st.caption("üî¥ Rouge: Identique (95-100%) | üü† Orange: Tr√®s √©lev√© (80-95%) | üü° Jaune: √âlev√© (70-80%)")
            
            # Graphique pour le fichier s√©lectionn√© (si filtr√©)
            if selected_filter != "Tous les r√©sultats" and len(filtered_df) > 1:
                st.markdown("---")
                st.subheader(f"üìä Graphique des similarit√©s pour {selected_filter}")
                
                fig_file = px.bar(
                    filtered_df,
                    x='other_file',
                    y='similarity_score',
                    title=f"Similarit√©s de {selected_filter} avec les autres fichiers",
                    labels={'other_file': 'Fichier', 'similarity_score': 'Score de similarit√©'},
                    color='similarity_score',
                    color_continuous_scale=['green', 'yellow', 'orange', 'red']
                )
                fig_file.add_hline(y=0.7, line_dash="dash", line_color="red", 
                                  annotation_text="Seuil de plagiat (70%)")
                st.plotly_chart(fig_file, use_container_width=True)
            
            # Tableau complet
            st.markdown("---")
            st.subheader("üìã R√©sultats complets")
            
            # Pr√©parer le dataframe d'affichage
            if selected_filter != "Tous les r√©sultats":
                display_full = filtered_df[['other_file', 'similarity_score']].copy()
                display_full.columns = ['Fichier compar√©', 'Score de similarit√©']
            else:
                display_full = filtered_df.copy()
            
            st.dataframe(display_full, use_container_width=True, hide_index=True)
            
            # Export
            csv_data = filtered_df.to_csv(index=False).encode('utf-8')
            filename_suffix = f"_{selected_filter}" if selected_filter != "Tous les r√©sultats" else "_all"
            st.download_button(
                label="üíæ T√©l√©charger les r√©sultats (CSV)",
                data=csv_data,
                file_name=f"plagiarism_results{filename_suffix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
    
    # ========================================
    # ONGLET 3: DOCUMENTATION
    # ========================================
    with tab3:
        st.header("üìñ Documentation")
        
        st.markdown("""
        ## üéØ Objectif
        
        Ce syst√®me d√©tecte automatiquement les similarit√©s entre des fichiers de code source
        en utilisant un algorithme distribu√© sur Apache Spark.
        
        ## üÜï Modes d'Analyse
        
        ### 1. Comparaison Compl√®te (All-to-All)
        - Compare **tous les fichiers** entre eux
        - G√©n√®re une matrice compl√®te de comparaisons
        - Id√©al pour d√©tecter le plagiat dans un groupe de soumissions
        - Complexit√©: O(n¬≤)
        
        ### 2. Comparaison Cibl√©e (Target vs Database)
        - Compare **un fichier sp√©cifique** avec tous les autres
        - Performance optimis√©e avec broadcast du fichier cible
        - Id√©al pour v√©rifier une soumission suspecte ou une nouvelle soumission
        - Complexit√©: O(n)
        
        **Cas d'usage typiques:**
        - üéì V√©rifier un √©tudiant suspect contre toute la base
        - üìß Analyser rapidement une pi√®ce jointe re√ßue par email
        - üÜï Tester un nouveau fichier contre l'historique complet
        
        ## üî¨ Algorithme
        
        ### 1. Extraction AST
        - Parsing du code source pour extraire la structure syntaxique
        - Support de Python, C++, et Java
        - Normalisation des tokens
        
        ### 2. Algorithme Winnowing
        - G√©n√©ration de k-grams (s√©quences de tokens)
        - Hashing de chaque k-gram
        - S√©lection des empreintes minimales par fen√™tre glissante
        - **Param√®tres**: k=5, window=4
        
        ### 3. Calcul de Similarit√©
        - Comparaison par paires de tous les fichiers
        - M√©trique: **Similarit√© de Jaccard**
        - Formule: J(A,B) = |A ‚à© B| / |A ‚à™ B|
        
        ## üìä Interpr√©tation des R√©sultats
        
        | Score | Cat√©gorie | Interpr√©tation |
        |-------|-----------|----------------|
        | 95-100% | Identique | Copie quasi-parfaite |
        | 80-95% | Tr√®s √©lev√©e | Plagiat probable |
        | 60-80% | √âlev√©e | Forte similarit√©, √† investiguer |
        | 40-60% | Mod√©r√©e | Similarit√© notable |
        | 20-40% | Faible | Peu de ressemblance |
        | 0-20% | Aucune | Codes distincts |
        
        ## üèóÔ∏è Architecture
        
        ```
        Client Streamlit
             ‚îÇ
             ‚ñº
        Spark Master ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ Worker 1
             ‚îÇ         ‚îî‚îÄ‚îÄ Worker 2
             ‚îÇ
             ‚ñº
          HDFS
        ```
        
        ## üîó Liens Utiles
        
        - [Spark Master UI](http://localhost:8080) - Monitoring du cluster
        - [HDFS NameNode UI](http://localhost:9870) - √âtat du syst√®me de fichiers
        
        ## üìö R√©f√©rences
        
        - **Winnowing Algorithm**: Schleimer, Wilkerson, Aiken (2003)
        - **Apache Spark**: https://spark.apache.org/
        - **HDFS**: https://hadoop.apache.org/
        """)


# ============================================
# POINT D'ENTR√âE
# ============================================
if __name__ == "__main__":
    main()
