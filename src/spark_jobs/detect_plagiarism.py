"""
============================================
SystÃ¨me de DÃ©tection de Plagiat DistribuÃ©
============================================
Job PySpark Principal - detect_plagiarism.py

Ce script implÃ©mente le pipeline complet de dÃ©tection :
1. Lecture des fichiers sources depuis HDFS
2. Extraction de l'AST (Abstract Syntax Tree)
3. GÃ©nÃ©ration d'empreintes digitales (Winnowing Algorithm)
4. Comparaison par paires (SimilaritÃ© Jaccard)
5. Sauvegarde des rÃ©sultats dans HDFS

Utilisation:
    spark-submit --master spark://spark-master:7077 \\
                 --deploy-mode client \\
                 detect_plagiarism.py \\
                 --input hdfs://spark-master:9000/app/input \\
                 --output hdfs://spark-master:9000/app/output
============================================
"""

import sys
import argparse
from datetime import datetime
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType, ArrayType, IntegerType, FloatType, StructType, StructField

# Import des modules locaux
sys.path.append('/app/src')
from spark_jobs.ast_extractor import ASTExtractor
from spark_jobs.winnowing import WinnowingHasher
from spark_jobs.similarity import SimilarityCalculator


class PlagiarismDetector:
    """
    Classe principale pour la dÃ©tection de plagiat distribuÃ©e.
    """
    
    def __init__(self, app_name="PlagiarismDetector", master_url="spark://spark-master:7077"):
        """
        Initialise la session Spark et les extracteurs.
        
        Args:
            app_name (str): Nom de l'application Spark
            master_url (str): URL du Spark Master
        """
        print(f"{'='*60}")
        print(f"ğŸš€ Initialisation du DÃ©tecteur de Plagiat")
        print(f"{'='*60}")
        
        # Configuration Spark
        self.conf = SparkConf() \
            .setAppName(app_name) \
            .setMaster(master_url) \
            .set("spark.executor.memory", "2g") \
            .set("spark.driver.memory", "1g") \
            .set("spark.executor.cores", "2") \
            .set("spark.default.parallelism", "4") \
            .set("spark.sql.shuffle.partitions", "4")
        
        # CrÃ©ation de la session Spark
        self.spark = SparkSession.builder.config(conf=self.conf).getOrCreate()
        self.sc = self.spark.sparkContext
        
        # Configuration du niveau de log
        self.sc.setLogLevel("WARN")
        
        print(f"âœ… SparkSession crÃ©Ã©e")
        print(f"   - Master: {master_url}")
        print(f"   - Application ID: {self.spark.sparkContext.applicationId}")
        print(f"   - ParallÃ©lisme: 4")
        
        # Initialisation des extracteurs
        self.ast_extractor = ASTExtractor()
        self.winnowing_hasher = WinnowingHasher(k=5, window_size=4)
        self.similarity_calculator = SimilarityCalculator(threshold=0.7)
        
        print(f"âœ… Extracteurs initialisÃ©s")
        print(f"{'='*60}\n")
    
    def read_source_files(self, input_path):
        """
        Lit les fichiers sources depuis HDFS.
        
        Args:
            input_path (str): Chemin HDFS des fichiers d'entrÃ©e
            
        Returns:
            RDD: RDD de tuples (filename, content)
        """
        print(f"ğŸ“‚ Lecture des fichiers depuis: {input_path}")
        
        try:
            # Lecture des fichiers texte
            # wholeTextFiles() retourne (filename, content)
            files_rdd = self.sc.wholeTextFiles(input_path)
            
            # Filtrer par extension (.py, .cpp, .java)
            def is_valid_file(filename):
                return filename.endswith(('.py', '.cpp', '.java', '.c', '.h'))
            
            files_rdd = files_rdd.filter(lambda x: is_valid_file(x[0]))
            
            file_count = files_rdd.count()
            print(f"âœ… {file_count} fichiers sources trouvÃ©s")
            
            # Afficher quelques exemples
            sample_files = files_rdd.take(3)
            print(f"\nğŸ“„ Exemples de fichiers:")
            for filename, _ in sample_files:
                print(f"   - {filename}")
            print()
            
            return files_rdd
            
        except Exception as e:
            print(f"âŒ Erreur lors de la lecture des fichiers: {e}")
            raise
    
    def extract_features(self, files_rdd):
        """
        Extrait les features (AST + Winnowing) de chaque fichier.
        
        Args:
            files_rdd (RDD): RDD de (filename, content)
            
        Returns:
            RDD: RDD de (filename, language, fingerprints)
        """
        print(f"ğŸ” Extraction des features (AST + Winnowing)...")
        
        # Broadcast des paramÃ¨tres seulement (pas d'objets)
        k_value = self.winnowing_hasher.k
        window_size = self.winnowing_hasher.window_size
        
        def extract_file_features(file_tuple):
            """
            Extrait les features d'un fichier.
            
            Args:
                file_tuple: (filename, content)
                
            Returns:
                tuple: (filename, language, fingerprints) ou None si erreur
            """
            filename, content = file_tuple
            
            try:
                # Imports locaux pour Ã©viter les problÃ¨mes de sÃ©rialisation
                import sys
                sys.path.append('/app/src')
                from spark_jobs.ast_extractor import ASTExtractor
                from spark_jobs.winnowing import WinnowingHasher
                
                # 1. DÃ©terminer le langage depuis l'extension
                if filename.endswith('.py'):
                    language = 'python'
                elif filename.endswith(('.cpp', '.c', '.h')):
                    language = 'cpp'
                elif filename.endswith('.java'):
                    language = 'java'
                else:
                    return None
                
                # 2. Extraire l'AST
                ast_extractor = ASTExtractor()
                ast_tokens = ast_extractor.extract_tokens(content, language)
                
                if not ast_tokens:
                    return (filename, language, [])
                
                # 3. GÃ©nÃ©rer les empreintes avec Winnowing
                winnowing_hasher = WinnowingHasher(k=k_value, window_size=window_size)
                fingerprints = winnowing_hasher.generate_fingerprints(ast_tokens)
                
                return (filename, language, fingerprints)
                
            except Exception as e:
                print(f"âš ï¸  Erreur pour {filename}: {e}")
                return (filename, 'unknown', [])
        
        # Extraction parallÃ¨le des features
        features_rdd = files_rdd.map(extract_file_features).filter(lambda x: x is not None)
        
        # Cache pour rÃ©utilisation
        features_rdd.cache()
        
        feature_count = features_rdd.count()
        print(f"âœ… Features extraites pour {feature_count} fichiers")
        
        # Statistiques
        sample_features = features_rdd.take(2)
        print(f"\nğŸ“Š Exemples de features:")
        for filename, language, fingerprints in sample_features:
            print(f"   - {filename}")
            print(f"     Langage: {language}")
            print(f"     Empreintes: {len(fingerprints)} hashes")
        print()
        
        return features_rdd
    
    def compare_pairs(self, features_rdd, target_file=None):
        """
        Compare tous les fichiers par paires pour dÃ©tecter les similaritÃ©s.
        
        Args:
            features_rdd (RDD): RDD de (filename, language, fingerprints)
            target_file (str): Fichier cible pour comparaison ciblÃ©e (optionnel)
            
        Returns:
            RDD: RDD de (file1, file2, similarity_score)
        """
        print(f"ğŸ”„ Comparaison par paires...")
        
        # Broadcast du seuil seulement (pas d'objets complexes)
        threshold = self.similarity_calculator.threshold
        
        if target_file:
            # Mode ciblÃ©: comparer le fichier target contre tous les autres
            print(f"   Mode ciblÃ©: {target_file} vs tous les autres")
            
            # Filtrer le fichier target
            target_rdd = features_rdd.filter(lambda x: target_file in x[0])
            target_data = target_rdd.collect()
            
            if not target_data:
                print(f"âŒ Erreur: Fichier cible '{target_file}' introuvable dans les donnÃ©es")
                return self.sc.emptyRDD()
            
            target_info = target_data[0]
            print(f"   âœ… Fichier cible trouvÃ©: {target_info[0]}")
            
            # Comparer le target contre tous les autres (sauf lui-mÃªme)
            other_files_rdd = features_rdd.filter(lambda x: target_file not in x[0])
            
            # Broadcast du fichier cible
            target_bc = self.sc.broadcast(target_info)
            
            def compare_with_target(other):
                """
                Compare le fichier cible avec un autre fichier.
                """
                target = target_bc.value
                file_target, lang_target, fp_target = target
                file_other, lang_other, fp_other = other
                
                # Ne comparer que les fichiers du mÃªme langage
                if lang_target != lang_other:
                    return (file_target, file_other, 0.0)
                
                # Calcul de la similaritÃ© Jaccard (sans dÃ©pendances)
                set_target = set(fp_target)
                set_other = set(fp_other)
                
                if not set_target or not set_other:
                    score = 0.0
                else:
                    intersection = len(set_target & set_other)
                    union = len(set_target | set_other)
                    score = intersection / union if union > 0 else 0.0
                
                return (file_target, file_other, score)
            
            similarities_rdd = other_files_rdd.map(compare_with_target)
            
        else:
            # Mode standard: crÃ©er toutes les paires possibles (cartÃ©sien)
            # Pour Ã©viter les doublons, on filtre file1 < file2
            print(f"   Mode standard: tous contre tous")
            pairs_rdd = features_rdd.cartesian(features_rdd) \
                .filter(lambda pair: pair[0][0] < pair[1][0])  # file1 < file2
        
            def calculate_similarity(pair):
                """
                Calcule la similaritÃ© entre deux fichiers.
                
                Args:
                    pair: ((file1, lang1, fp1), (file2, lang2, fp2))
                    
                Returns:
                    tuple: (file1, file2, similarity_score)
                """
                (file1, lang1, fp1), (file2, lang2, fp2) = pair
                
                # Ne comparer que les fichiers du mÃªme langage
                if lang1 != lang2:
                    return (file1, file2, 0.0)
                
                # Calcul de la similaritÃ© Jaccard (sans dÃ©pendances)
                set1 = set(fp1)
                set2 = set(fp2)
                
                if not set1 or not set2:
                    score = 0.0
                else:
                    intersection = len(set1 & set2)
                    union = len(set1 | set2)
                    score = intersection / union if union > 0 else 0.0
                
                return (file1, file2, score)
            
            # Calcul parallÃ¨le des similaritÃ©s
            similarities_rdd = pairs_rdd.map(calculate_similarity)
        
        # Filtrer les similaritÃ©s significatives (> 0.1)
        significant_similarities = similarities_rdd.filter(lambda x: x[2] > 0.1)
        
        # Trier par score dÃ©croissant
        sorted_similarities = significant_similarities.sortBy(lambda x: -x[2])
        
        print(f"âœ… Comparaisons terminÃ©es")
        
        return sorted_similarities
    
    def save_results(self, similarities_rdd, output_path):
        """
        Sauvegarde les rÃ©sultats dans HDFS.
        
        Args:
            similarities_rdd (RDD): RDD de (file1, file2, similarity_score)
            output_path (str): Chemin HDFS de sortie
        """
        print(f"ğŸ’¾ Sauvegarde des rÃ©sultats dans: {output_path}")
        
        try:
            # Convertir en DataFrame pour un meilleur format
            schema = StructType([
                StructField("file1", StringType(), False),
                StructField("file2", StringType(), False),
                StructField("similarity_score", FloatType(), False)
            ])
            
            df = self.spark.createDataFrame(similarities_rdd, schema)
            
            # Ajouter un timestamp
            from pyspark.sql.functions import lit, current_timestamp
            df = df.withColumn("detection_time", current_timestamp())
            
            # Sauvegarder en CSV
            df.coalesce(1).write.mode("overwrite").csv(
                output_path,
                header=True
            )
            
            # Afficher les top rÃ©sultats
            print(f"\nğŸ† Top 10 des similaritÃ©s dÃ©tectÃ©es:")
            top_10 = df.orderBy(col("similarity_score").desc()).limit(10)
            top_10.show(truncate=False)
            
            print(f"âœ… RÃ©sultats sauvegardÃ©s avec succÃ¨s")
            
        except Exception as e:
            print(f"âŒ Erreur lors de la sauvegarde: {e}")
            raise
    
    def run(self, input_path, output_path, target_file=None):
        """
        ExÃ©cute le pipeline complet de dÃ©tection.
        
        Args:
            input_path (str): Chemin HDFS des fichiers d'entrÃ©e
            output_path (str): Chemin HDFS de sortie
            target_file (str): Fichier cible Ã  comparer (optionnel)
        """
        start_time = datetime.now()
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ DÃ‰MARRAGE DU PIPELINE DE DÃ‰TECTION")
        print(f"{'='*60}")
        print(f"â° Heure de dÃ©but: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“¥ Input: {input_path}")
        print(f"ğŸ“¤ Output: {output_path}")
        if target_file:
            print(f"ğŸ¯ Mode: Comparaison ciblÃ©e - fichier: {target_file}")
        else:
            print(f"ğŸ¯ Mode: Comparaison tous contre tous")
        print(f"{'='*60}\n")
        
        try:
            # Ã‰tape 1: Lecture des fichiers
            files_rdd = self.read_source_files(input_path)
            
            # Ã‰tape 2: Extraction des features
            features_rdd = self.extract_features(files_rdd)
            
            # Ã‰tape 3: Comparaison par paires
            similarities_rdd = self.compare_pairs(features_rdd, target_file)
            
            # Ã‰tape 4: Sauvegarde des rÃ©sultats
            self.save_results(similarities_rdd, output_path)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            print(f"\n{'='*60}")
            print(f"âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS")
            print(f"{'='*60}")
            print(f"â° Heure de fin: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â±ï¸  DurÃ©e totale: {duration:.2f} secondes")
            print(f"{'='*60}\n")
            
        except Exception as e:
            print(f"\n{'='*60}")
            print(f"âŒ ERREUR DURANT L'EXÃ‰CUTION")
            print(f"{'='*60}")
            print(f"Message: {e}")
            print(f"{'='*60}\n")
            raise
        
        finally:
            # Nettoyage
            self.spark.stop()
            print("ğŸ›‘ SparkSession fermÃ©e\n")


def main():
    """
    Point d'entrÃ©e principal du script.
    """
    # Configuration des arguments en ligne de commande
    parser = argparse.ArgumentParser(
        description="DÃ©tection de plagiat de code source avec Spark"
    )
    parser.add_argument(
        "--input",
        type=str,
        default="hdfs://spark-master:9000/app/input",
        help="Chemin HDFS des fichiers d'entrÃ©e"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="hdfs://spark-master:9000/app/output",
        help="Chemin HDFS de sortie"
    )
    parser.add_argument(
        "--master",
        type=str,
        default="spark://spark-master:7077",
        help="URL du Spark Master"
    )
    parser.add_argument(
        "--target",
        type=str,
        default=None,
        help="Fichier cible Ã  comparer contre tous les autres (chemin HDFS complet ou nom de fichier)"
    )
    
    args = parser.parse_args()
    
    # CrÃ©ation et exÃ©cution du dÃ©tecteur
    detector = PlagiarismDetector(master_url=args.master)
    detector.run(args.input, args.output, target_file=args.target)


if __name__ == "__main__":
    main()
