# ============================================
# Makefile - Commandes Rapides
# ============================================
# Utilisation: make <commande>
# Exemple: make start
# ============================================

.PHONY: help build start stop restart logs clean status test upload-examples run-job

# Par dÃ©faut, afficher l'aide
help:
	@echo "================================================"
	@echo "ğŸ” SystÃ¨me de DÃ©tection de Plagiat - Commandes"
	@echo "================================================"
	@echo ""
	@echo "ğŸ“¦ Gestion du Cluster:"
	@echo "  make build          - Build les images Docker"
	@echo "  make start          - DÃ©marrer le cluster"
	@echo "  make stop           - ArrÃªter le cluster"
	@echo "  make restart        - RedÃ©marrer le cluster"
	@echo "  make clean          - Tout supprimer (images + volumes)"
	@echo ""
	@echo "ğŸ“Š Monitoring:"
	@echo "  make status         - Voir l'Ã©tat des conteneurs"
	@echo "  make logs           - Voir les logs (tous les services)"
	@echo "  make logs-master    - Logs du Spark Master"
	@echo "  make logs-worker1   - Logs du Worker 1"
	@echo "  make logs-client    - Logs du Client"
	@echo ""
	@echo "ğŸ§ª Tests:"
	@echo "  make test           - Test de connexion Spark"
	@echo "  make upload-examples - Upload les exemples vers HDFS"
	@echo "  make run-job        - Lancer une analyse de plagiat"
	@echo ""
	@echo "ğŸŒ URLs:"
	@echo "  Streamlit:    http://localhost:8501"
	@echo "  Spark Master: http://localhost:8080"
	@echo "  HDFS:         http://localhost:9870"
	@echo ""

# Build des images Docker
build:
	@echo "ğŸ”¨ Build des images Docker..."
	docker-compose build

# DÃ©marrer le cluster
start:
	@echo "ğŸš€ DÃ©marrage du cluster..."
	docker-compose up -d
	@echo "âœ… Cluster dÃ©marrÃ©!"
	@echo ""
	@echo "ğŸŒ AccÃ©der aux interfaces:"
	@echo "  - Streamlit:    http://localhost:8501"
	@echo "  - Spark Master: http://localhost:8080"
	@echo "  - HDFS:         http://localhost:9870"

# ArrÃªter le cluster
stop:
	@echo "ğŸ›‘ ArrÃªt du cluster..."
	docker-compose down
	@echo "âœ… Cluster arrÃªtÃ©!"

# RedÃ©marrer le cluster
restart: stop start

# Tout nettoyer (conteneurs + volumes + images)
clean:
	@echo "ğŸ§¹ Nettoyage complet..."
	docker-compose down -v
	docker-compose rm -f
	@echo "âœ… Nettoyage terminÃ©!"

# Supprimer tout (y compris les images)
clean-all: clean
	@echo "ğŸ—‘ï¸  Suppression des images..."
	docker rmi -f $$(docker images 'codeplagiat*' -q) 2>/dev/null || true
	@echo "âœ… Images supprimÃ©es!"

# Voir l'Ã©tat des conteneurs
status:
	@echo "ğŸ“Š Ã‰tat des conteneurs:"
	@docker-compose ps

# Voir tous les logs
logs:
	docker-compose logs -f --tail=100

# Logs du Master
logs-master:
	docker-compose logs -f spark-master

# Logs du Worker 1
logs-worker1:
	docker-compose logs -f spark-worker-1

# Logs du Worker 2
logs-worker2:
	docker-compose logs -f spark-worker-2

# Logs du Client
logs-client:
	docker-compose logs -f app-client

# Test de connexion Spark
test:
	@echo "ğŸ§ª Test de connexion Spark..."
	docker exec spark-master pyspark --version
	@echo "âœ… Test rÃ©ussi!"

# Upload des exemples vers HDFS
upload-examples:
	@echo "ğŸ“¤ Upload des exemples vers HDFS..."
	docker exec spark-master hdfs dfs -mkdir -p /app/input
	docker exec spark-master hdfs dfs -put -f /app/data/input/*.py /app/input/
	@echo "âœ… Exemples uploadÃ©s!"
	@echo ""
	@echo "ğŸ“‹ Fichiers dans HDFS:"
	docker exec spark-master hdfs dfs -ls /app/input

# Lancer un job d'analyse
run-job:
	@echo "ğŸ” Lancement du job d'analyse..."
	docker exec spark-master spark-submit \
		--master spark://spark-master:7077 \
		/app/src/spark_jobs/detect_plagiarism.py \
		--input hdfs://spark-master:9000/app/input \
		--output hdfs://spark-master:9000/app/output
	@echo "âœ… Job terminÃ©!"

# Shell interactif dans le Master
shell-master:
	@echo "ğŸš Shell dans le Spark Master..."
	docker exec -it spark-master bash

# Shell interactif dans le Worker 1
shell-worker1:
	@echo "ğŸš Shell dans le Worker 1..."
	docker exec -it spark-worker-1 bash

# Shell PySpark interactif
pyspark:
	@echo "ğŸ Lancement de PySpark..."
	docker exec -it spark-master pyspark --master spark://spark-master:7077

# Lister les fichiers HDFS
hdfs-ls:
	@echo "ğŸ“ Contenu de HDFS:"
	docker exec spark-master hdfs dfs -ls -R /app

# VÃ©rifier la santÃ© du systÃ¨me
health:
	@echo "ğŸ’š VÃ©rification de la santÃ© du systÃ¨me..."
	@echo ""
	@echo "ğŸ³ Conteneurs Docker:"
	@docker-compose ps
	@echo ""
	@echo "âš¡ Spark Workers:"
	@docker exec spark-master curl -s http://localhost:8080/json/ | grep -o '"aliveworkers":[0-9]*' || echo "N/A"
	@echo ""
	@echo "ğŸ“ HDFS:"
	@docker exec spark-master hdfs dfsadmin -report | grep -E "Live datanodes|Configured Capacity" || echo "N/A"

# Ouvrir les URLs dans le navigateur (Windows)
open-urls:
	@echo "ğŸŒ Ouverture des interfaces..."
	start http://localhost:8501
	start http://localhost:8080
	start http://localhost:9870

# Installation rapide (premiÃ¨re utilisation)
install: build start upload-examples
	@echo ""
	@echo "================================================"
	@echo "âœ… Installation terminÃ©e avec succÃ¨s!"
	@echo "================================================"
	@echo ""
	@echo "ğŸ‰ Le systÃ¨me est prÃªt Ã  l'emploi!"
	@echo ""
	@echo "ğŸŒ AccÃ©der Ã  l'application:"
	@echo "   http://localhost:8501"
	@echo ""
	@echo "ğŸ“š Voir le guide de dÃ©marrage:"
	@echo "   cat QUICKSTART.md"
	@echo ""
