# ============================================
# Configuration Spark par Défaut
# ============================================
# Fichier de configuration pour Apache Spark.
# Ces paramètres s'appliquent à toutes les applications
# soumises au cluster.
# ============================================

# ============================================
# CONFIGURATION DU CLUSTER
# ============================================

# URL du Spark Master
spark.master                    spark://spark-master:7077

# Nom de l'application (par défaut)
spark.app.name                  PlagiarismDetector

# ============================================
# CONFIGURATION DES RESSOURCES
# ============================================

# Mémoire allouée aux Executors
# Chaque worker aura des executors avec cette mémoire
spark.executor.memory           2g

# Mémoire allouée au Driver
# Le driver coordonne l'exécution de l'application
spark.driver.memory             1g

# Nombre de cœurs par Executor
spark.executor.cores            2

# Nombre total d'executors
spark.executor.instances        2

# ============================================
# PARALLELISME ET PARTITIONS
# ============================================

# Niveau de parallélisme par défaut pour les RDDs
# Recommandé: 2-3 fois le nombre total de cœurs
spark.default.parallelism       4

# Nombre de partitions pour les shuffles SQL
spark.sql.shuffle.partitions    4

# ============================================
# EVENT LOGS (Pour History Server)
# ============================================

# Activer la journalisation des événements
spark.eventLog.enabled          true

# Répertoire de stockage des logs d'événements
spark.eventLog.dir              file:///tmp/spark-events

# Compresser les logs d'événements
spark.eventLog.compress         true

# ============================================
# HISTORY SERVER
# ============================================

# Répertoire des logs pour History Server
spark.history.fs.logDirectory   file:///tmp/spark-events

# Port du History Server
spark.history.ui.port           18080

# ============================================
# SPARK UI
# ============================================

# Port de l'interface Web du Driver
spark.ui.port                   4040

# Conserver les applications terminées dans l'UI
spark.ui.retainedJobs           100
spark.ui.retainedStages         100
spark.ui.retainedTasks          1000

# ============================================
# CONFIGURATION RÉSEAU
# ============================================

# Timeout pour les connexions réseau (secondes)
spark.network.timeout           120s

# Timeout RPC (secondes)
spark.rpc.askTimeout            120s

# Nombre de tentatives de connexion
spark.port.maxRetries           5

# ============================================
# SÉRIALISATION
# ============================================

# Utiliser Kryo pour de meilleures performances
# (Commenté car nécessite l'enregistrement des classes)
# spark.serializer              org.apache.spark.serializer.KryoSerializer

# ============================================
# GESTION DE LA MÉMOIRE
# ============================================

# Fraction de mémoire pour l'exécution
spark.memory.fraction           0.6

# Fraction de mémoire pour le storage
spark.memory.storageFraction    0.5

# ============================================
# CONFIGURATION PYTHON
# ============================================

# Chemins Python pour les workers
spark.executorEnv.PYTHONPATH    /app/src

# Version de Python
spark.pyspark.python            python3

# ============================================
# SQL ET DATAFRAMES
# ============================================

# Répertoire warehouse pour Spark SQL
spark.sql.warehouse.dir         /tmp/spark-warehouse

# Broadcast automatique des petites tables (10 MB)
spark.sql.autoBroadcastJoinThreshold    10485760

# Adaptive Query Execution (Spark 3.x)
spark.sql.adaptive.enabled      true

# ============================================
# CONFIGURATION HDFS
# ============================================

# URL HDFS par défaut
spark.hadoop.fs.defaultFS       hdfs://spark-master:9000

# ============================================
# LOGS ET DEBUG
# ============================================

# Niveau de log par défaut
spark.driver.extraJavaOptions   -Dlog4j.configuration=file:///opt/spark/conf/log4j.properties

# Afficher les erreurs Python complètes
spark.python.worker.reuse       true

# ============================================
# OPTIMISATIONS
# ============================================

# Compression des données intermédiaires
spark.io.compression.codec      snappy

# Activer la compression des RDDs en mémoire
spark.rdd.compress              true

# Spéculation (relancer les tâches lentes)
spark.speculation               false

# ============================================
# LIMITES ET QUOTAS
# ============================================

# Taille maximale des résultats collectés par le driver
spark.driver.maxResultSize      1g

# Taille maximale d'un message réseau
spark.rpc.message.maxSize       128
