# ğŸ—ï¸ Architecture DÃ©taillÃ©e - SystÃ¨me de DÃ©tection de Plagiat

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Composants SystÃ¨me](#composants-systÃ¨me)
3. [Pipeline de Traitement](#pipeline-de-traitement)
4. [Algorithmes](#algorithmes)
5. [Configuration](#configuration)
6. [Performance et ScalabilitÃ©](#performance-et-scalabilitÃ©)
7. [SÃ©curitÃ©](#sÃ©curitÃ©)

---

## ğŸ¯ Vue d'ensemble

### Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COUCHE CLIENT                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚        Application Streamlit (Port 8501)           â”‚  â”‚
â”‚  â”‚  - Upload de fichiers                              â”‚  â”‚
â”‚  â”‚  - Soumission de jobs                              â”‚  â”‚
â”‚  â”‚  - Visualisation des rÃ©sultats                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COUCHE SPARK                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚       Spark Master (spark-master:7077)             â”‚  â”‚
â”‚  â”‚  - Orchestration des jobs                          â”‚  â”‚
â”‚  â”‚  - Allocation des ressources                       â”‚  â”‚
â”‚  â”‚  - Monitoring (UI: 8080)                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                         â”‚                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Worker 1:8081   â”‚      â”‚  Worker 2:8082  â”‚          â”‚
â”‚  â”‚  - 2 cores       â”‚      â”‚  - 2 cores      â”‚          â”‚
â”‚  â”‚  - 2GB RAM       â”‚      â”‚  - 2GB RAM      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COUCHE STOCKAGE                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          HDFS (hdfs://spark-master:9000)           â”‚  â”‚
â”‚  â”‚                                                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  NameNode    â”‚    â”‚     DataNodes (x2)       â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  (Metadata)  â”‚â—„â”€â”€â”€â”¤  - Stockage distribuÃ©    â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  UI: 9870    â”‚    â”‚  - RÃ©plication: 1        â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                     â”‚  â”‚
â”‚  â”‚  Structure:                                         â”‚  â”‚
â”‚  â”‚  /app/input/  - Fichiers sources                   â”‚  â”‚
â”‚  â”‚  /app/output/ - RÃ©sultats d'analyse                â”‚  â”‚
â”‚  â”‚  /app/logs/   - Logs systÃ¨me                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Composants SystÃ¨me

### 1. Spark Master

**RÃ´le:** Orchestrateur central du cluster Spark

**ResponsabilitÃ©s:**
- Recevoir les jobs soumis
- Allouer les ressources aux applications
- Coordonner les Workers
- Fournir l'interface de monitoring

**SpÃ©cifications:**
- **CPU:** Variable (partagÃ© avec NameNode)
- **RAM:** 1GB pour le Driver
- **Ports:**
  - 7077: Communication Spark
  - 8080: WebUI
  - 9000: HDFS NameNode RPC
  - 9870: HDFS NameNode WebUI

**Configuration clÃ©:**
```properties
spark.master=spark://spark-master:7077
spark.driver.memory=1g
spark.executor.instances=2
```

### 2. Spark Workers

**RÃ´le:** NÅ“uds de calcul pour exÃ©cuter les tÃ¢ches

**Configuration par Worker:**
- **Cores:** 2
- **Memory:** 2GB
- **Executor Memory:** 2GB par executor
- **Ports:**
  - Worker 1: 8081 (WebUI), 7078 (Communication)
  - Worker 2: 8082 (WebUI), 7079 (Communication)

**CapacitÃ© totale du cluster:**
- **Total Cores:** 4
- **Total Memory:** 4GB
- **ParallÃ©lisme:** 4 tÃ¢ches simultanÃ©es

### 3. HDFS

**Architecture:**

```
NameNode (Master)
â”œâ”€â”€ Metadata Storage (/hdfs/namenode)
â”œâ”€â”€ Namespace Management
â””â”€â”€ Block Locations

DataNodes (2x)
â”œâ”€â”€ Block Storage (/hdfs/datanode)
â”œâ”€â”€ Heartbeat vers NameNode
â””â”€â”€ RÃ©plication: 1 copie
```

**Configuration:**
- **Block Size:** 128MB
- **Replication Factor:** 1 (dev) / 3 (prod)
- **WebHDFS:** ActivÃ© (API REST)

### 4. Client Streamlit

**Technologies:**
- **Framework:** Streamlit 1.28.1
- **Backend:** Python 3.9
- **DÃ©pendances:**
  - PySpark (communication)
  - Plotly (visualisation)
  - Pandas (manipulation de donnÃ©es)

**FonctionnalitÃ©s:**
- Upload de fichiers vers HDFS
- Soumission de jobs Spark
- Visualisation des rÃ©sultats
- Export des rapports

---

## ğŸ”„ Pipeline de Traitement

### Ã‰tape 1: Ingestion des DonnÃ©es

```python
# Lecture depuis HDFS
files_rdd = sc.wholeTextFiles("hdfs://spark-master:9000/app/input")
# RDD[(filename, content)]

# Filtrage par extension
valid_files = files_rdd.filter(lambda x: x[0].endswith(('.py', '.cpp', '.java')))
```

**Optimisations:**
- Lecture parallÃ¨le distribuÃ©e
- Filtrage prÃ©coce pour rÃ©duire le volume
- Cache des donnÃ©es pour rÃ©utilisation

### Ã‰tape 2: Extraction AST

```python
def extract_ast(file_tuple):
    filename, content = file_tuple
    
    # 1. DÃ©tection du langage
    language = detect_language(filename)
    
    # 2. Parsing AST
    tokens = ast_extractor.extract_tokens(content, language)
    
    # 3. Normalisation
    normalized_tokens = normalize(tokens)
    
    return (filename, language, normalized_tokens)

features_rdd = valid_files.map(extract_ast)
```

**Techniques:**
- Tokenization native Python pour `.py`
- Regex-based parsing pour C++/Java (simplifiÃ©)
- Option: Tree-sitter pour parsing robuste

### Ã‰tape 3: GÃ©nÃ©ration d'Empreintes (Winnowing)

```python
def generate_fingerprints(tokens):
    # 1. CrÃ©er les k-grams (k=5)
    kgrams = [tokens[i:i+5] for i in range(len(tokens)-4)]
    
    # 2. Hasher chaque k-gram
    hashes = [hash_kgram(kg) for kg in kgrams]
    
    # 3. Winnowing (window=4)
    fingerprints = []
    for i in range(len(hashes)-3):
        window = hashes[i:i+4]
        min_hash = min(window)
        fingerprints.append(min_hash)
    
    return fingerprints

fp_rdd = features_rdd.map(lambda x: (x[0], x[1], generate_fingerprints(x[2])))
```

**ParamÃ¨tres Winnowing:**
- **k=5**: SÃ©quence de 5 tokens
- **window=4**: FenÃªtre glissante de 4 hashes
- **Garantie:** DÃ©tecte tout match de longueur â‰¥ k+window-1 = 8 tokens

### Ã‰tape 4: Comparaison par Paires

```python
# CartÃ©sien pour toutes les paires
pairs = fp_rdd.cartesian(fp_rdd)

# Ã‰viter les doublons et auto-comparaisons
pairs = pairs.filter(lambda p: p[0][0] < p[1][0])

def compare(pair):
    (file1, _, fp1), (file2, _, fp2) = pair
    
    # SimilaritÃ© Jaccard
    intersection = len(set(fp1) & set(fp2))
    union = len(set(fp1) | set(fp2))
    similarity = intersection / union if union > 0 else 0
    
    return (file1, file2, similarity)

results = pairs.map(compare)
```

**ComplexitÃ©:**
- **Paires:** O(nÂ²) oÃ¹ n = nombre de fichiers
- **Comparaison:** O(m) oÃ¹ m = taille moyenne des sets
- **Total:** O(nÂ² * m)

### Ã‰tape 5: Filtrage et Tri

```python
# Filtrer les similaritÃ©s significatives
significant = results.filter(lambda x: x[2] > 0.1)

# Trier par score dÃ©croissant
sorted_results = significant.sortBy(lambda x: -x[2])

# Top 100 rÃ©sultats
top_results = sorted_results.take(100)
```

### Ã‰tape 6: Sauvegarde

```python
# Convertir en DataFrame
df = spark.createDataFrame(sorted_results, ["file1", "file2", "similarity"])

# Ajouter mÃ©tadonnÃ©es
df = df.withColumn("timestamp", current_timestamp())

# Sauvegarder en CSV
df.coalesce(1).write.csv(output_path, header=True, mode="overwrite")
```

---

## ğŸ§® Algorithmes

### Algorithme de Winnowing

**Papier de rÃ©fÃ©rence:**
> Schleimer, S., Wilkerson, D. S., & Aiken, A. (2003).  
> *Winnowing: Local Algorithms for Document Fingerprinting*  
> ACM SIGMOD Conference

**Principe:**

1. **K-gramming:**
   ```
   Input: [a, b, c, d, e, f]  (k=3)
   Output: [(a,b,c), (b,c,d), (c,d,e), (d,e,f)]
   ```

2. **Hashing:**
   ```
   (a,b,c) â†’ H1 = 45
   (b,c,d) â†’ H2 = 12
   (c,d,e) â†’ H3 = 78
   (d,e,f) â†’ H4 = 34
   ```

3. **Windowing (w=2):**
   ```
   Window 1: [H1=45, H2=12] â†’ min=12 âœ“
   Window 2: [H2=12, H3=78] â†’ min=12 (dÃ©jÃ  pris)
   Window 3: [H3=78, H4=34] â†’ min=34 âœ“
   
   Fingerprints: [12, 34]
   ```

**Garantie:**
- Si deux documents partagent une sous-sÃ©quence de longueur â‰¥ (k + w - 1), au moins une empreinte commune sera dÃ©tectÃ©e.

**Trade-offs:**
- k petit â†’ plus sensible, plus d'empreintes
- k grand â†’ moins sensible, moins d'empreintes
- w petit â†’ plus d'empreintes, dÃ©tection fine
- w grand â†’ moins d'empreintes, performances

### SimilaritÃ© de Jaccard

**Formule:**
```
J(A, B) = |A âˆ© B| / |A âˆª B|
```

**PropriÃ©tÃ©s:**
- **SymÃ©trique:** J(A,B) = J(B,A)
- **BornÃ©:** 0 â‰¤ J(A,B) â‰¤ 1
- **IdentitÃ©:** J(A,A) = 1
- **Disjoint:** J(A,B) = 0 si A âˆ© B = âˆ…

**Exemple:**
```python
A = {1, 2, 3, 4, 5}
B = {3, 4, 5, 6, 7}

Intersection = {3, 4, 5}  â†’ |A âˆ© B| = 3
Union = {1, 2, 3, 4, 5, 6, 7}  â†’ |A âˆª B| = 7

J(A,B) = 3/7 â‰ˆ 0.43 (43% de similaritÃ©)
```

---

## âš™ï¸ Configuration

### Spark Configuration (spark-defaults.conf)

**Ressources:**
```properties
spark.executor.memory=2g         # MÃ©moire par executor
spark.driver.memory=1g           # MÃ©moire du driver
spark.executor.cores=2           # Cores par executor
spark.default.parallelism=4      # Niveau de parallÃ©lisme
```

**Performance:**
```properties
spark.sql.shuffle.partitions=4   # Partitions pour shuffles
spark.io.compression.codec=snappy # Compression
spark.rdd.compress=true          # Compression RDD
```

**RÃ©seau:**
```properties
spark.network.timeout=120s       # Timeout rÃ©seau
spark.rpc.askTimeout=120s        # Timeout RPC
```

### HDFS Configuration (hdfs-site.xml)

**Stockage:**
```xml
<property>
    <name>dfs.replication</name>
    <value>1</value>  <!-- Dev: 1, Prod: 3 -->
</property>

<property>
    <name>dfs.blocksize</name>
    <value>134217728</value>  <!-- 128 MB -->
</property>
```

**RÃ©seau:**
```xml
<property>
    <name>dfs.namenode.rpc-address</name>
    <value>spark-master:9000</value>
</property>
```

---

## ğŸ“ˆ Performance et ScalabilitÃ©

### MÃ©triques de Performance

**Temps d'ExÃ©cution (estimÃ©):**
| Nombre de Fichiers | Taille Moy. | Temps Total |
|--------------------|-------------|-------------|
| 10                 | 100 lignes  | ~10 sec     |
| 50                 | 100 lignes  | ~45 sec     |
| 100                | 100 lignes  | ~3 min      |
| 500                | 100 lignes  | ~20 min     |

**Facteurs d'influence:**
- ComplexitÃ© du code (nombre de tokens)
- Nombre de comparaisons (nÂ²)
- RÃ©seau entre conteneurs
- I/O HDFS

### Optimisations

**1. Augmenter le ParallÃ©lisme:**
```yaml
# docker-compose.yml
environment:
  - SPARK_WORKER_CORES=4    # Au lieu de 2
  - SPARK_WORKER_MEMORY=4g  # Au lieu de 2g
```

**2. Ajouter des Workers:**
```yaml
spark-worker-3:
  # Configuration identique aux autres workers
  environment:
    - SPARK_WORKER_PORT=7080
    - SPARK_WORKER_WEBUI_PORT=8083
```

**3. Optimiser les Partitions:**
```python
# Repartitionner selon le nombre de cores
files_rdd = files_rdd.repartition(num_workers * cores_per_worker)
```

**4. Utiliser le Cache:**
```python
# Mettre en cache les RDDs rÃ©utilisÃ©s
features_rdd.cache()
features_rdd.count()  # DÃ©clencher le cache
```

### ScalabilitÃ©

**Horizontale (Ajouter des Workers):**
- âœ… LinÃ©aire pour le traitement des fichiers
- âœ… Quasi-linÃ©aire pour l'extraction AST
- âš ï¸ LimitÃ© par le cartÃ©sien (comparaison)

**Verticale (Plus de Ressources):**
- âœ… Plus de mÃ©moire â†’ plus de cache
- âœ… Plus de cores â†’ plus de parallÃ©lisme
- âš ï¸ Rendements dÃ©croissants au-delÃ  de 8 cores

**Recommandations Production:**
- **Petit dÃ©ploiement (< 100 fichiers):**
  - 1 Master + 2 Workers (4 cores, 4GB chacun)
  
- **Moyen dÃ©ploiement (100-1000 fichiers):**
  - 1 Master + 5 Workers (8 cores, 8GB chacun)
  
- **Large dÃ©ploiement (> 1000 fichiers):**
  - 1 Master + 10+ Workers (16+ cores, 16GB+ chacun)
  - Activer Kryo serialization
  - Optimiser les shuffles

---

## ğŸ”’ SÃ©curitÃ©

### Environnement de DÃ©veloppement

**Configurations actuelles (NON pour production):**
- âŒ Permissions HDFS dÃ©sactivÃ©es
- âŒ Authentification Spark dÃ©sactivÃ©e
- âŒ Pas de SSL/TLS
- âŒ Ports exposÃ©s publiquement

### Recommandations Production

**1. HDFS Security:**
```xml
<property>
    <name>dfs.permissions.enabled</name>
    <value>true</value>
</property>

<property>
    <name>hadoop.security.authentication</name>
    <value>kerberos</value>
</property>
```

**2. Spark Security:**
```properties
spark.authenticate=true
spark.authenticate.secret=<secret_key>
spark.network.crypto.enabled=true
spark.ssl.enabled=true
```

**3. Docker Network:**
```yaml
networks:
  spark-network:
    driver: bridge
    internal: true  # Isoler du monde extÃ©rieur
```

**4. Reverse Proxy:**
```nginx
# nginx.conf
server {
    listen 443 ssl;
    server_name plagiarism.example.com;
    
    location / {
        proxy_pass http://app-client:8501;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

---

## ğŸ“š RÃ©fÃ©rences

### Technologies
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Hadoop HDFS Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)
- [Streamlit Documentation](https://docs.streamlit.io/)

### Algorithmes
- Winnowing Paper: [Schleimer et al., 2003](https://theory.stanford.edu/~aiken/publications/papers/sigmod03.pdf)
- Jaccard Similarity: [Wikipedia](https://en.wikipedia.org/wiki/Jaccard_index)

### DÃ©tection de Plagiat
- MOSS (Measure Of Software Similarity)
- JPlag
- SIM Software Similarity Tester

---

**Version:** 1.0  
**DerniÃ¨re mise Ã  jour:** November 2025  
**Auteur:** CodePlagiat Project
